{
 "metadata": {
  "name": "",
  "signature": "sha256:16c40b2855e38d5e430f56e386b11f62a7651ad89905e021d256a509d1602b2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import h5py\n",
      "import random\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(tf.__version__)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.10.0rc0\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadH5(fname):\n",
      "    with h5py.File(fname, 'r') as f:\n",
      "        X = f['data'][:]\n",
      "        X = np.array(X).astype(np.float32)\n",
      "        y = f['label'][:]\n",
      "        y = np.array(y).astype(np.float32)\n",
      "        y_ = f['realy'][:]\n",
      "        y_ = np.array(y_).astype(np.float32)\n",
      "    return X, y, y_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y, y_ = loadH5(\"./caffe/data_0.h5\")\n",
      "Xt, y, y_ = loadH5(\"./caffe/data_1.h5\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#X = X/100000.\n",
      "print X.shape, Xt.shape\n",
      "print X, X.max(), X.min()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(5052451,) (1263113,)\n",
        "[ 16.          -3.          -8.         ...,  -0.80000001  -3.20000005\n",
        "  -0.60000002] 212.0 -167.1\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess = tf.InteractiveSession()\n",
      "tf.device(\"/gpu:1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<contextlib.GeneratorContextManager at 0x7f5babd3b710>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def variable_summaries(var, prefix=\"prefix_\"):\n",
      "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
      "    #with tf.name_scope(\"v_sum_\"+prefix):\n",
      "    mean = tf.reduce_mean(var)\n",
      "    tf.scalar_summary(prefix+'mean', mean)\n",
      "    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
      "    tf.scalar_summary(prefix+'stddev', stddev)\n",
      "    tf.scalar_summary(prefix+'max', tf.reduce_max(var))\n",
      "    tf.scalar_summary(prefix+'min', tf.reduce_min(var))\n",
      "    tf.histogram_summary(prefix+'histogram', var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_st = tf.placeholder(tf.float32, [None, 1024, 1, 1])\n",
      "keep_prob = tf.placeholder(tf.float32)\n",
      "rmask = tf.random_uniform([1024], 0, 1)\n",
      "dmask = tf.to_float(rmask <= keep_prob)\n",
      "tmask = tf.to_float(rmask > keep_prob)\n",
      "print dmask, tmask, rmask"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tensor(\"ToFloat:0\", dtype=float32) Tensor(\"ToFloat_1:0\", dtype=float32) Tensor(\"random_uniform:0\", shape=(1024,), dtype=float32)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.name_scope(\"aegen_\"):\n",
      "    input_st_reshape_ = tf.reshape(input_st, [-1, 1024])\n",
      "    #input_st_reshape = (input_st_reshape_ - tf.reduce_mean(input_st_reshape_)) * 255.\n",
      "    input_st_reshape = input_st_reshape_\n",
      "    input_x = input_st_reshape * dmask\n",
      "    \n",
      "    w1 = tf.Variable(tf.truncated_normal([1024,2048], stddev=0.1), name=\"w1\")\n",
      "    b1 = tf.Variable(tf.constant(0.1, shape=[2048]), name=\"b1\")\n",
      "    l1 = tf.nn.relu(tf.matmul(input_x,w1) + b1)\n",
      "    \n",
      "    w2 = tf.Variable(tf.truncated_normal([2048,2048], stddev=0.1), name=\"w2\")\n",
      "    b2 = tf.Variable(tf.constant(0.1, shape=[2048]), name=\"b2\")\n",
      "    l2 = tf.nn.relu(tf.matmul(l1,w2) + b2)\n",
      "\n",
      "    w4 = tf.Variable(tf.truncated_normal([2048,1024], stddev=0.1), name=\"w4\")\n",
      "    b4 = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b4\")\n",
      "    l4 = tf.matmul(l2,w4) + b4\n",
      "    \n",
      "    #input_x_norm = (input_st_reshape - tf.reduce_min(input_st_reshape)) / \\\n",
      "    #    (tf.reduce_max(input_st_reshape) - tf.reduce_min(input_st_reshape))\n",
      "    #input_x_norm = tf.nn.l2_normalize(input_st_reshape_, 1)\n",
      "    input_x_norm = input_st_reshape_\n",
      "    x_t = input_x_norm * tmask\n",
      "    l_t = l4 * tmask\n",
      "    sim_loss = tf.reduce_mean(tf.square(l_t - x_t))\n",
      "    l4_sig = l4\n",
      "    #sim_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(l_t, x_t))\n",
      "    #l4_sig = tf.nn.sigmoid(l4)\n",
      "    \n",
      "    # now calc the norm drop\n",
      "    idiff = input_x_norm - l4_sig\n",
      "    imean = tf.reduce_mean(idiff, 0)\n",
      "    istd = tf.sqrt(tf.reduce_mean(tf.square(idiff - imean), 0))\n",
      "    inorm1 = (istd - tf.reduce_min(istd)) / (tf.reduce_max(istd) - tf.reduce_min(istd))\n",
      "    #inorm = inorm1 * inorm1\n",
      "    ilog = -tf.log(inorm1 + 0.000000001) # choose the std -> min\n",
      "    inorm = (ilog - tf.reduce_min(ilog)) / (tf.reduce_max(ilog) - tf.reduce_min(ilog))\n",
      "    inorm = 1-inorm # choose the std -> max\n",
      "    print imean, istd\n",
      "    \n",
      "    # drop the inputs\n",
      "    x_trans = input_x_norm * inorm\n",
      "    #x_in_ = x_trans - tf.reduce_mean(x_trans, 0)\n",
      "    x_in_ = x_trans\n",
      "    \n",
      "    # now batch norm for the next input\n",
      "    #in_mean = tf.reduce_mean(x_trans, 0)\n",
      "    #x_in_ = (x_trans - in_mean) * 255. / \\\n",
      "    #    (tf.sqrt(tf.reduce_mean(tf.square(x_trans - in_mean), 0)) + 0.000000001)\n",
      "    x_in = tf.reshape(x_in_, [-1, 1024, 1, 1])\n",
      "    print x_in\n",
      "\n",
      "sess.run(tf.initialize_all_variables())\n",
      "_ = sess.run(input_x_norm, feed_dict={\n",
      "    input_st: np.random.random([128, 1024, 1, 1]),\n",
      "    keep_prob: 0.5\n",
      "})\n",
      "print _.shape, _.max(), _.min() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tensor(\"aegen_/Mean_1:0\", shape=(1024,), dtype=float32) Tensor(\"aegen_/Sqrt:0\", shape=(1024,), dtype=float32)\n",
        "Tensor(\"aegen_/Reshape_1:0\", shape=(?, 1024, 1, 1), dtype=float32)\n",
        "(128, 1024)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.999987 2.5164e-07\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Qnet(input_data, kdrop, again=False):\n",
      "    with tf.variable_scope(\"param\"):\n",
      "        #wc1 = tf.get_variable(\"wc1\", [17, 1, 1, 64], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
      "        wc1 = tf.get_variable(\"wc1\", [17, 1, 1, 64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
      "        wb1 = tf.get_variable(\"wb1\", [64], initializer=tf.constant_initializer(0.1))\n",
      "        c1 = tf.nn.relu(tf.nn.conv2d(\n",
      "            input_data, wc1, \n",
      "            strides=[1, 2, 1, 1], \n",
      "            padding='SAME') + wb1)\n",
      "        \n",
      "        wc2 = tf.get_variable(\"wc2\", [2, 1, 64, 64], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
      "        wb2 = tf.get_variable(\"wb2\", [64], initializer=tf.constant_initializer(0.1))\n",
      "        c2 = tf.nn.relu(tf.nn.conv2d(\n",
      "            c1, wc2, \n",
      "            strides=[1, 1, 1, 1], \n",
      "            padding='SAME') + wb2)\n",
      "\n",
      "        flat = tf.reshape(c2, [-1, 512 * 64])\n",
      "        print c1, c2, flat\n",
      "\n",
      "        w1 = tf.get_variable(\"w1\", [512 * 64, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
      "        b1 = tf.get_variable(\"b1\", [1024], initializer=tf.constant_initializer(0.1))\n",
      "        l1 = tf.nn.relu(tf.matmul(flat, w1) + b1)\n",
      "        d1 = tf.nn.dropout(l1, kdrop)\n",
      "        \n",
      "        w2 = tf.get_variable(\"w2\", [1024, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
      "        b2 = tf.get_variable(\"b2\", [1024], initializer=tf.constant_initializer(0.1))\n",
      "        l2 = tf.nn.relu(tf.matmul(d1, w2) + b2)\n",
      "        d2 = tf.nn.dropout(l2, kdrop)\n",
      "\n",
      "        W = tf.get_variable(\"W\", [1024, 60], initializer=tf.contrib.layers.xavier_initializer())\n",
      "        B = tf.get_variable(\"B\", [60], initializer=tf.constant_initializer(0.1))\n",
      "        Q = tf.matmul(d2, W) + B\n",
      "        if again:\n",
      "            tf.histogram_summary('l1', l1)\n",
      "            tf.histogram_summary('l2', l2)\n",
      "            #variable_summaries(wc1, \"wc1\")\n",
      "            #variable_summaries(wb1, \"wb1\")\n",
      "            variable_summaries(wc2, \"wc2\")\n",
      "            variable_summaries(wb2, \"wb2\")\n",
      "            #variable_summaries(w1, \"w1\")\n",
      "            #variable_summaries(b1, \"b1\")\n",
      "            #variable_summaries(w2, \"w2\")\n",
      "            #variable_summaries(b2, \"b2\")\n",
      "    return Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GAMA = 0.9\n",
      "up_down = tf.placeholder(tf.float32, [None, 2, 1])\n",
      "time_choose = tf.placeholder(tf.float32, [None, 1, 30])\n",
      "reward = tf.placeholder(tf.float32, [None])\n",
      "input_st_next = tf.placeholder(tf.float32, [None, 1024, 1, 1])\n",
      "\n",
      "with tf.variable_scope(\"qnet_\") as scope:\n",
      "    Q = Qnet(x_in, keep_prob)\n",
      "\n",
      "    Q_reshape = tf.reshape(Q, [-1, 2, 30])\n",
      "    Q_value_mat = Q_reshape * up_down * time_choose\n",
      "    Q_value = tf.reduce_sum(tf.reduce_sum(Q_value_mat, 2), 1)\n",
      "    \n",
      "    scope.reuse_variables()\n",
      "    #print dir(scope)\n",
      "    Q_ = Qnet(input_st_next, keep_prob, True)\n",
      "    #next_act = tf.one_hot(tf.argmax(Q_, 1), depth=60, axis=1)\n",
      "    nextQ = tf.reduce_max(Q_, 1)\n",
      "    print nextQ\n",
      "    loss = tf.reduce_mean(tf.square(Q_value - (reward + GAMA * nextQ)))\n",
      "    tf.scalar_summary('loss', loss)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tensor(\"qnet_/param/Relu:0\", shape=(?, 512, 1, 64), dtype=float32) Tensor(\"qnet_/param/Relu_1:0\", shape=(?, 512, 1, 64), dtype=float32) Tensor(\"qnet_/param/Reshape:0\", shape=(?, 32768), dtype=float32)\n",
        "Tensor(\"qnet_/param_1/Relu:0\", shape=(?, 512, 1, 64), dtype=float32) Tensor(\"qnet_/param_1/Relu_1:0\", shape=(?, 512, 1, 64), dtype=float32) Tensor(\"qnet_/param_1/Reshape:0\", shape=(?, 32768), dtype=float32)\n",
        "Tensor(\"qnet_/Max:0\", shape=(?,), dtype=float32)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_vars = tf.trainable_variables()\n",
      "aevars = [var for var in t_vars if 'aegen_' in var.name]\n",
      "qvars = [var for var in t_vars if 'qnet_' in var.name]\n",
      "print [var.name for var in aevars]\n",
      "print [var.name for var in qvars]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'aegen_/w1:0', u'aegen_/b1:0', u'aegen_/w2:0', u'aegen_/b2:0', u'aegen_/w4:0', u'aegen_/b4:0']\n",
        "[u'qnet_/param/wc1:0', u'qnet_/param/wb1:0', u'qnet_/param/wc2:0', u'qnet_/param/wb2:0', u'qnet_/param/w1:0', u'qnet_/param/b1:0', u'qnet_/param/w2:0', u'qnet_/param/b2:0', u'qnet_/param/W:0', u'qnet_/param/B:0']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list=qvars)\n",
      "ae_step = tf.train.AdamOptimizer(1e-3).minimize(sim_loss, var_list=aevars)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_vars = tf.trainable_variables()\n",
      "print len(t_vars)\n",
      "print [_.name for _ in t_vars]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16\n",
        "[u'aegen_/w1:0', u'aegen_/b1:0', u'aegen_/w2:0', u'aegen_/b2:0', u'aegen_/w4:0', u'aegen_/b4:0', u'qnet_/param/wc1:0', u'qnet_/param/wb1:0', u'qnet_/param/wc2:0', u'qnet_/param/wb2:0', u'qnet_/param/w1:0', u'qnet_/param/b1:0', u'qnet_/param/w2:0', u'qnet_/param/b2:0', u'qnet_/param/W:0', u'qnet_/param/B:0']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess.run(tf.initialize_all_variables())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "define the batch loaders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def genDatas(pri_data, bsize=32, ftlen=1024, ftrlen=60):\n",
      "    crr = np.zeros((bsize, ftlen)).astype(np.float32)\n",
      "    ftr = np.zeros((bsize, ftrlen)).astype(np.float32)\n",
      "    datalen = pri_data.shape[0]\n",
      "    for i in range(bsize):\n",
      "        rand_i = random.randint(0, datalen-1-ftlen-ftrlen)\n",
      "        crr[i] = pri_data[rand_i:rand_i+ftlen]\n",
      "        ftr[i] = pri_data[rand_i+ftlen:rand_i+ftlen+ftrlen]\n",
      "    return (crr).reshape(-1, ftlen, 1, 1), (ftr).reshape(-1, ftrlen, 1, 1)\n",
      "\n",
      "def nextInfo(ud, t, crr, ftr):\n",
      "    reward_v = np.zeros(ud.shape[0]).astype(np.float32)\n",
      "    next_state = np.zeros((ud.shape[0], crr.shape[1])).astype(np.float32)\n",
      "    for i in range(ud.shape[0]):\n",
      "        indice = t[i].argmax()\n",
      "        walk = ftr[i][0:indice].sum()\n",
      "        walk = walk if ud[i][0] > ud[i][1] else (walk * -1)\n",
      "        #walk = 0 if walk < 0 else walk\n",
      "        reward_v[i] = walk\n",
      "        #print crr[i][indice+1:].shape\n",
      "        #print ftr[i][0:indice+1].shape\n",
      "        next_state[i] = np.concatenate(\n",
      "            [crr[i][indice+1:], ftr[i][0:indice+1]]\n",
      "            , axis=0).reshape(-1)\n",
      "    return reward_v, next_state.reshape(-1, crr.shape[1], 1, 1)\n",
      "    \n",
      "test = genDatas(X)\n",
      "print test[0].shape, test[1].shape\n",
      "#print test[0], test[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(32, 1024, 1, 1) (32, 60, 1, 1)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "INITIAL_EPSILON = 0.5\n",
      "FINAL_EPSILON = 0.01\n",
      "loop_max = 10000000\n",
      "epsilon = INITIAL_EPSILON\n",
      "\n",
      "def egreedy_action(state, is_test=False):\n",
      "    global epsilon, sess, Q, INITIAL_EPSILON, FINAL_EPSILON, loop_max\n",
      "    updown_choose_v = np.zeros((state.shape[0], 2))\n",
      "    t_choose_v = np.zeros((state.shape[0], 30))\n",
      "    Q_v = sess.run(Q, feed_dict = {input_st: state, keep_prob:1.})\n",
      "    Q_v = Q_v.reshape(-1, 2, 30)\n",
      "    for i in range(state.shape[0]):\n",
      "        if random.random() <= epsilon and not is_test: \n",
      "            updown_choose_v[i][random.randint(0,1)] = 1\n",
      "            t_choose_v[i][random.randint(5,29)] = 1\n",
      "        else: \n",
      "            indices = Q_v[i].argmax()\n",
      "            if indices <= 29:\n",
      "                updown_choose_v[i][0] = 1\n",
      "                t_choose_v[i][indices] = 1\n",
      "            else:\n",
      "                updown_choose_v[i][1] = 1\n",
      "                t_choose_v[i][indices - 30] = 1\n",
      "                \n",
      "    if not is_test:\n",
      "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/loop_max\n",
      "    return updown_choose_v, t_choose_v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Xt.shape\n",
      "def tester():\n",
      "    psum = abs_psum = 0\n",
      "    num = 512\n",
      "    for i in range(Xt.shape[0]/num):\n",
      "        crr, ftr = genDatas(Xt, bsize=num)\n",
      "        ud, t = egreedy_action(crr, True)\n",
      "        r, next_st = nextInfo(ud, t, crr, ftr)\n",
      "        psum += r.sum()\n",
      "        abs_psum += abs(r).sum()\n",
      "        if i % num == 0:\n",
      "            print \".\",\n",
      "    return psum, abs_psum\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1263113,)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "checkpoint_dir = \"./ts_snapshot/\"\n",
      "log_dir = \"/root/logs/tf\"\n",
      "saver = tf.train.Saver()\n",
      "train_writer = tf.train.SummaryWriter(log_dir + '/train', sess.graph)\n",
      "merged = tf.merge_all_summaries()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(loop_max):\n",
      "    crr, ftr = genDatas(X, bsize=128)\n",
      "    ud, t = egreedy_action(crr)\n",
      "    r, next_st = nextInfo(ud, t, crr, ftr)\n",
      "    #print next_st.shape\n",
      "    #break\n",
      "    ud.shape = (-1, 2, 1)\n",
      "    t.shape = (-1, 1, 30)\n",
      "    _, sloss = sess.run([ae_step, sim_loss], feed_dict={\n",
      "        input_st: crr,\n",
      "        keep_prob:0.8,\n",
      "    })\n",
      "    \n",
      "    _, summary, loss_v = sess.run([train_step, merged, loss], feed_dict={\n",
      "        input_st: crr,\n",
      "        up_down: ud,\n",
      "        time_choose: t,\n",
      "        reward: r,\n",
      "        input_st_next: next_st,\n",
      "        keep_prob:0.9,\n",
      "    })\n",
      "    if i > 0:\n",
      "        if i % 2 == 0:\n",
      "            train_writer.add_summary(summary, i)\n",
      "        if i % 1000 == 0:\n",
      "            print i, loss_v, sloss\n",
      "        if i % 10000 == 0:\n",
      "            psum, abs_psum = tester()\n",
      "            print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum \n",
      "            save_path = saver.save(sess, checkpoint_dir+\"model_%d.ckpt\"%i)\n",
      "            print \"Model saved in file: \", save_path\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "psum, abs_psum = tester()\n",
      "print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum \n",
      "psum, abs_psum = tester()\n",
      "print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum \n",
      "psum, abs_psum = tester()\n",
      "print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum \n",
      "psum, abs_psum = tester()\n",
      "print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum \n",
      "psum, abs_psum = tester()\n",
      "print \"test:\", psum, abs_psum, psum * 1.0 / abs_psum "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create tmp(0) checkpoint\n",
      "i = 0\n",
      "saver.save(sess, checkpoint_dir+\"model_%d.ckpt\"%i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2, restore training\n",
      "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
      "if ckpt and ckpt.model_checkpoint_path:\n",
      "    print \"restoring the training\", ckpt.model_checkpoint_path\n",
      "saver.restore(sess, ckpt.model_checkpoint_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TODO LIST:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "1, maybe ud action should be optmized"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}